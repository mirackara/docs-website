---
title: "What's causing my app's high latency?"
tags:
- Learning paths
- Customer experience
- Browser monitoring
metaDescription: "How to find, diagnose, and fix, an instance of your abnormal or poor app latency."
redirects:
---

import solutionsOmaUprPatternNormal from 'images/solutions_screenshot-full_oma-upr-pattern-normal.png'

import solutionsNormalPercentilePattern from 'images/solutions_screenshot-full_normal-percentile-pattern.png'

import solutionsPatternAbnormal from 'images/solutions_screenshot-full_pattern-abnormal.png'

import solutionsPatternAbnormalCompare from 'images/solutions_screenshot-full_pattern-abnormal-compare.png'

Your app's performance is one of the most important aspects of whether or not it will succeed. Up to 70% of mobile users will avoid using an application if it takes too long to load, and up to 53% of website users will do the same for a site they visit. Diagnosing and fixing performance issues can help your organization attract new users while retaining existing ones, and New Relic is just the tool to help you do it. 

The terms "performance", "latency", and "response time" encompass multiple issues. The majority of "slowness" problems occur due to an output issue stemming from backend services. New Relic's service level management measures this slowness for both "output" and "client" categories. 

## Diagnostic steps [#diagnostic-steps]
<Steps>

<Step>
First, you should establish the problem statement in as simple and effective a way as possible. A good problem statement answers the following questions:

1. What is the problem that the end-user is experiencing?
2. What is it that the end-user should be experiencing?
3. What is the technical assessment of what the user is experiencing?
</Step>

<Step>
There are three primary breakpoint categories to improve your response times in identifying the source of an issue:

1. **Output**
2. **Input**
3. **Client**

Defining your performance metrics within these categories, also called [service levels](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide), significantly improves your response time finding the source of a problem. We cover measuring these categories in [our service level management guide](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide).
</Step>

<Step>
The next step after identifying the source of a problem is to identify what changed. This helps you determine how to resolve the problem quickly. Examples of common changes are:

1. Throughput (traffic)
2. Code (deployments)
3. Resources (hardware allocation)
4. Upstream or downstream dependency changes
5. Data volume
</Step>
</Steps>

Let's look at an example problem. Let's say you deploy a new product, and a significant increase in requests causes unacceptable response times. The source is discovered in the login middleware service. The problem is a jump in TCP queue times.

Here's a breakdown of this situation:

* **Category**: output performance
* **Source**: login middleware
* **Direct cause**: TCP queue times from additional request load
* **Solution**: increased TCP connection limit and scaled resources
* **Root-cause**: insufficient capacity planning and quality assurance testing on downstream service impacting login middleware

## Understand performance categories [#health-data-points]

As previously mentioned, there are three primary performance categories that jump-start your diagnostic journey. Understanding these significantly reduces your time to understanding the source of the problem.

<CollapserGroup>
  <Collapser
    id="output-perf"
    title="Output performance"
  >

**This requires**: APM

Output performance is the ability of your internal technology stack to deliver the expected responses (output) to the end-user. This is traditionally referred to as the "back-end" services. Most of the time, output performance is measured by the speed and quality of the response. The end-user describes the service as either slow, not working, or inaccessible.

The most common obstacle with output performance issues is the ability to respond to end-user requests in a timely **and** successful manner, but you can address this by identifying a latency anomaly or error anomaly.
</Collapser>
  <Collapser
    id="input-perf"
    title="Input performance"
  >

**This requires**: Synthetics

Input performance is the ability of your services to receive requests from the client. Errors result when something between the client and your services is breaking the request-response lifecycle, even when your output performance could be exceeding expected performance levels. This could occur at any point between the client and your services.
</Collapser>
  <Collapser
    id="client-perf"
    title="Client performance"
  >
**This requires**: Browser monitoring and/or mobile monitoring

Client performance is the ability for a browser and/or mobile application to make requests and render responses. You can identify browser and/or mobile issues as the source of the problem once both output (back-end) and input performance (synthetics) have been ruled out. Due to the depth of diagnostics in input and output diagnostic, browser and mobile will be covered in an advanced diagnostics guide in the future.
</Collapser>
</CollapserGroup>

## Identifying performance pattern anomalies [#pattern-anomalies]

<Callout variant="tip">
Having well-formed [service levels](/docs/new-relic-solutions/observability-maturity/uptime-performance-reliability/optimize-slm-guide) on your service boundaries relating to key transactions (capabilities) will help you more quickly identify what end-to-end workflow the problem resides in.
</Callout>

Identifying pattern anomalies will improve your ability to identify what and where the direct cause of problems may be.

There's plenty of great information and free online classes on identifying patterns, but the general concept is rather simple and can unlock powerful diagnostic abilities.

The key to identifying patterns and anomalies in performance data is that you don't need to know how the service should be performing: **You only need to determine if the recent behavior has changed.**

The examples provided in this section use response time or latency as the metric but you can apply the same analysis to almost any dataset, such as errors, throughput, hardware resource metrics, queue depths, and more.

<CollapserGroup>
  <Collapser
    className="freq-link"
    id="normal"
    title="Normal"
  >
Below is an example of a seemingly volatile response time chart (7 days) in APM. Looking closer, you can see that the behavior of the response time is repetitive. In other words, there's no radical change in the behavior across a 7-day period. The spikes are repetitive and not unusual compared to the rest of the timeline.

<img
  alt="normal pattern"
  title="Normal pattern"
  src={solutionsOmaUprPatternNormal}
/>

In fact, if you change the view of the data from **average over time** to **percentiles over time**, it becomes even more clear how "regular" the changes in response time are.

<img
  alt="normal pattern with percentile"
  title="Normal pattern with percentile"
  src={solutionsNormalPercentilePattern}
/>
  </Collapser>

  <Collapser
    className="freq-link"
    id="abnormal"
    title="Abnormal"
  >
This chart shows an application response time that seems to have increased in an unusual way compared to recent behavior.

<img
  alt="abnormal pattern"
  title="Abnormal pattern"
  src={solutionsPatternAbnormal}

/>

This can be confirmed by using the week-over-week comparison.

<img
  alt="abnormal pattern week-over-week"
  title="Abnormal pattern week-over-week comparison"
  src={solutionsPatternAbnormalCompare}
/>

We see that the pattern has changed and that it appears to be worsening from last week's comparison.
  </Collapser>
</CollapserGroup>